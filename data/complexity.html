Сложность алгоритма
<p>Предположим, что вы умеете создавать алгоритмы. Однако хочу вас огорчить. Не все алгоритмы, которые в теории решают проблему хороши на практике. Есть понятие вычислительной сложности алгоритма.</p>
<p>В математике есть «O» большое, которое является верхней границей скорости роста некой функции. Мы будем использовать его для ограничения скорости роста «количества» вычислительных операций.</p>
<p>Рассмотрим различные «сложности» для того, чтобы понять, что это такое.</p>
<p>Например, ваш алгоритм принимает два числа и складывает их. Если записать этот алгоритм в виде кода и скомпилировать его, это будет какое-то количество команд ассемблера. Но там не будет циклов, поэтому наша программа будет выполнена за этих несколько операций и завершится. Предположим, что для этого нам нужно лишь 3 операции. Т.е. мы можем ограничить сверху выполнение нашей программы тремя операциями, вот так: O(3). Однако при обозначении сложности количество операций может отличаться между разными компиляторами/ассемблерами/компьютерами. Поэтому O(3) умножается на константу C, которая означает «коэфициент конкретности» (сам название придумал, не запоминайте), который может быть разным и мы его не знаем. Чуть дальше об этом.</p>
<p>И суть такова, что мы можем вынести константу из-под O и «поглотить» её нашей виртуальной константой C (она ведь всё равно нам неизвестная и может изменяться, какая разница, как она будет меняться?). В итоге получается, что O(3) = O(1). Это значит, что наша программа выполнится за константное время. На одном компьютере это будут мили/микро/пикосекунды, на другом, например, это может быть час (например, выполняются какие-нибудь другие программы и планировщик просто не даёт времени нашему коду).</p>
<p>Теперь возьмём другой случай. У нас есть список длины N и мы проходимся по нему два раза и применяем на каждый его элемент каких-то 5 операций. Т.е. у нас будет O(2 * 5 * N), верно? Однако 10 является константой и может быть убрано, в итоге получается O(N). Что значит, что при размере входных данных (в данном случае длине списка) равным N наша программа выполнится за линейное время. И если прибавить ещё несколько элементов в список — то и время увеличится на константу умноженную на количество добавленных элементов.</p>
<p>Перейдём ещё к более крутым сложностям. Теперь сделаем вложенный цикл, в котором будем проходиться по элементам списка (опять же длиной в N), во внешнем цикле так же будем проходиться по ним же. Т.е. для каждого элемента из списка мы будем получать все остальные и его самого. Здесь наше O будет равно O(N^2). И смотрите. Если мы добавим несколько элементов — время увеличится уже не линейно, а примерно квадратично.</p>
<p>А теперь предположим, что одна часть программы у нас имеет линейную сложность, а другая квадратичную. Получается O(N + N^2). Однако тут действует хитрое «правило». Если мы устремим N к бесконечности, то N будет расти намного медленнее, чем N^2 и практически не будет оказывать влияния на итоговое время. Поэтому мы можем отбросить его. Так, O(N + N^2) = O(N^2).</p>
<p>Для того, чтобы рассчитывать сложность своих алгоритмов необходим опыт, но простые случаи можно легко видеть. Если вы проходитесь по чему-то один раз — это O(N), если два раза — всё равно O(N). Но если вы сделали три вложенных цикла — это уже O(N^3).</p>
<p>Так же существуют и другие классы сложностей, вроде O(N^3), O(2^N), O(log N), O(N log N).</p>
<p>Для чего это нужно? Смотрите. Допустим, вы написали сортировку таким образом, что каждый элемент проверяете с каждым. Это получается O(N^2). А Вася написал сортировку, которая имеет O(N log N). Т.е. N умножить на логарифм (не суть важно основание) N. И при N → ∞, ваше N^2 будет выглядеть... медленно, по сравнению с N log N. Потому что N log N растёт куда медленнее. Если не верите — нарисуйте график того и другого. А время — деньги. При увеличении данных вам придётся покупать куда больше ресурсов, чем Васе, хотя его алгоритм выполняет ту же задачу, что и ваш. Только лучше.</p>
